{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reads in the data and shows an example\n",
    "Don't worry about calling this more than once, despite the fact that this is a huge dataset, the dataframe only stores the caption and a link to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"Train_GCC-training.tsv\",sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To show an example, we need to make a request to the web to download the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = training_data.head(1)[1][0]\n",
    "print(image_url)\n",
    "image_filename = wget.download(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mpimg.imread(image_filename)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a funciton to write images to an S3 bucket \n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "\n",
    "# made a typo when creating the bucket :)\n",
    "bucket = \"caption-images-cs231m\"\n",
    "\n",
    "def open_and_upload_file(file_name, counter):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    \"\"\"\n",
    "    # get image\n",
    "    image_filename = \"\" \n",
    "    try: \n",
    "        image_filename = wget.download(file_name)\n",
    "    except:\n",
    "        print(\"failed\")\n",
    "        return False\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(image_filename, bucket, \"training/\" + str(counter))\n",
    "    # delete file locally after\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    os.remove(image_filename)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a little confirmation test lol\n",
    "res = open_and_upload_file(training_data.head(1)[1][0], 0)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this on a thread so we can kill any downloads that take longer than 3 seconds\n",
    "import multiprocessing\n",
    "import time\n",
    "counter = 50230\n",
    "for img in training_data.loc[50230:].iterrows():\n",
    "    p = multiprocessing.Process(target=open_and_upload_file, args=(img[1][1], counter))\n",
    "    p.start()\n",
    "    p.join(3)\n",
    "    # If thread is still active\n",
    "    if p.is_alive():\n",
    "        print(\"running... let's kill it...\")\n",
    "\n",
    "        # Terminate - may not work if process is stuck for good\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "    \n",
    "    if (counter % 10 == 0):\n",
    "        print(str(counter) + \" processed \")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell downloads the images to an s3 bucket! much easier than having it locally\n",
    "counter = 0\n",
    "for img in training_data.iterrows():\n",
    "    res = open_and_upload_file(img[1][1], counter)\n",
    "    if (counter % 10 == 0):\n",
    "        print(str(counter) + \" processed with status \" + str(res))\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in small amount of dataset:\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "bucket_name = \"caption-images-cs231m\"\n",
    "\n",
    "def read_image_from_s3(filename):\n",
    "    \"\"\"Load image file from s3.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bucket: string\n",
    "        Bucket name\n",
    "    key : string\n",
    "        Path in s3\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np array\n",
    "        Image array\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    object = bucket.Object(filename)\n",
    "    response = object.get()\n",
    "    file_stream = response['Body']\n",
    "    im = Image.open(file_stream)\n",
    "    return np.array(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images from the bucket & reshape to correct size\n",
    "def load_image(image_path):\n",
    "    img = tf.convert_to_tensor(read_image_from_s3(str(image_path)))\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to make sure the function above works\n",
    "img = load_image(\"training/21271\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get all our beautiful data into a list of captions and img_names :)\n",
    "skip the ones that have processing issues~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucket_name)\n",
    "image_names = []\n",
    "captions = []\n",
    "images = []\n",
    "processing_counter = 0\n",
    "for item in my_bucket.objects.filter(Prefix='training/'):\n",
    "    img = []\n",
    "    try:\n",
    "        img = load_image(item.key)\n",
    "    except:\n",
    "        print(\"could not read\", item.key)\n",
    "        continue\n",
    "    processing_counter += 1\n",
    "    images.append(img)\n",
    "    image_names.append(str(item.key))\n",
    "    caption_index = int(item.key.split(\"/\")[1])\n",
    "    captions.append(training_data[0][caption_index])\n",
    "    if processing_counter % 100 == 0:\n",
    "        print(\"currently processing image number \", processing_counter)\n",
    "    if processing_counter == 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that sizes match\n",
    "print(len(image_names), len(captions), len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pretrained imagenet weights\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of batches to use\n",
    "num_batches = int((len(image_names) - 3840 )/ 64)\n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[3756 + 53] = images[3564 + 53][:,:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare tensors in batches\n",
    "for i in range(9500, 10000, 64):\n",
    "    start = i\n",
    "    end = i + 64\n",
    "    print(\"processing batch with start \", start, \"and end \", end)\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(images[start:end])\n",
    "    image_names_dataset = tf.data.Dataset.from_tensor_slices(image_names[start:end])\n",
    "    image_dataset_with_path = tf.data.Dataset.zip((image_dataset, image_names_dataset))\n",
    "    for img, path in image_dataset_with_path:\n",
    "        new_image = tf.expand_dims(img,0)\n",
    "        batch_features = image_features_extract_model(new_image)\n",
    "        batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "        for bf in batch_features:\n",
    "            path_of_feature = path.numpy().decode(\"utf-8\")\n",
    "            np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# figure out which images didn't get processsed..........\n",
    "names_set = image_names\n",
    "processed = os.listdir(\"training/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_file(word):\n",
    "    words = word.split(\".\")\n",
    "    return words[0]\n",
    "    \n",
    "def get_num_s3(word):\n",
    "    words = word.split(\"/\")\n",
    "    return words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names_set = set(map(get_num_s3, names_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = set(map(get_num_file, processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = names_set - processed\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"captions\", captions)\n",
    "np.save(\"image_names\", image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images are done processing (everything above only needs to be run once okay!)\n",
    "##### now we process the captions with some tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load captions and image names here if you haven't already\n",
    "captions = np.load(\"captions.npy\")\n",
    "image_names = np.load(\"image_names.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add start and end to captions\n",
    "training_captions = []\n",
    "for c in captions:\n",
    "    training_captions.append(f\"<start> {c} <end>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(training_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "tokenizer.word_index['<mask>'] = top_k\n",
    "tokenizer.index_word[top_k] = '<mask>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize captions\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(training_captions)\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing\n",
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(image_names, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "def pretrain_map_func(img_name, cap, y):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i in range(len(img_name_train))]\n",
    "vis_img_name_pretrain = []\n",
    "vis_cap_pretrain      = []\n",
    "vis_y_pretrain        = []\n",
    "for (i, (img_name, cap)) in enumerate(zip(img_name_train, cap_train)):\n",
    "    randIndex = np.random.choice(indices)\n",
    "    randIndex = indices[(indices.index(i) + 1) % len(indices)] if randIndex == i else randIndex\n",
    "    #add false example to pretrain\n",
    "    vis_img_name_pretrain.append(img_name_train[randIndex])\n",
    "    vis_cap_pretrain.append(cap_train[randIndex])\n",
    "    vis_y_pretrain.append(np.array([0]).astype(\"int32\"))\n",
    "\n",
    "    #add positive example to pretrain\n",
    "    vis_img_name_pretrain.append(img_name)\n",
    "    vis_cap_pretrain.append(cap)\n",
    "    vis_y_pretrain.append(np.array([1]).astype(\"int32\"))\n",
    "    \n",
    "    #remove the index\n",
    "    indices.remove(randIndex)\n",
    "\n",
    "visPretrainDataset = tf.data.Dataset.from_tensor_slices((vis_img_name_pretrain, vis_cap_pretrain, vis_y_pretrain))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "visPretrainDataset = visPretrainDataset.map(lambda item1, item2, item3: tf.numpy_function(\n",
    "          pretrain_map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.int32])\n",
    "          )\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32])\n",
    "          )\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(BUFFER_SIZE)\n",
    "visPretrainDataset = visPretrainDataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "visPretrainDataset = visPretrainDataset.prefetch(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, v_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(v_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(v_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "vis_classifier = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_pretrain_step(img_tensor, caption, y):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=caption.shape[0])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "        \n",
    "        for i in range(caption.shape[1]):\n",
    "            _, hidden, _ = decoder(caption[:, i:i+1], features, hidden)\n",
    "                \n",
    "        prediction = vis_classifier(hidden)\n",
    "            \n",
    "        loss = visual_loss_function(y, prediction)\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables \\\n",
    "              + decoder.trainable_variables + vis_classifier.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, cap, y)) in enumerate(visPretrainDataset):\n",
    "        t_loss = visual_pretrain_step(img_tensor, cap, y)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = t_loss\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_pretrain_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            print(target)\n",
    "            # passing the features through the decoder\n",
    "            token_input = tf.where(tf.random.uniform((target.shape[0],1)) < .15, vocab_size-1, vocab_size)\n",
    "            predictions, hidden, _ = decoder(token_input, features, hidden)\n",
    "                \n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = language_pretrain_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's train... please work!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch,\n",
    " (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_local(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image, from_bucket=True):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image), 0) if from_bucket else tf.expand_dims(load_image_local(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
    "                                                 -1,\n",
    "                                                 img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot,from_bucket=True):\n",
    "    temp_image = np.array(read_image_from_s3(image)) if from_bucket else np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for i in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
    "        grid_size = max(np.ceil(len_result/2), 2)\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
    "        ax.set_title(result[i])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i]\n",
    "                        for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print('Real Caption:', real_caption)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
    "image_extension = image_url[-4:]\n",
    "image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)\n",
    "\n",
    "result, attention_plot = evaluate(image_path, False)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image_path, result, attention_plot, False)\n",
    "# opening the image\n",
    "Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to calculate BLEU score for a caption\n",
    "import nltk\n",
    "\n",
    "def BLEU_4(ref_caption, gen_caption):\n",
    "    return nltk.translate.bleu_score.sentence_bleu([ref_caption], gen_caption)\n",
    "\n",
    "def BLEU_3(ref_caption, gen_caption):\n",
    "    return nltk.translate.bleu_score.sentence_bleu([ref_caption], gen_caption,  weights=(0, 0, 1, 0))\n",
    "\n",
    "def BLEU_2(ref_caption, gen_caption):\n",
    "    return nltk.translate.bleu_score.sentence_bleu([ref_caption], gen_caption,  weights=(0, 1, 0, 0))\n",
    "\n",
    "def BLEU_1(ref_caption, gen_caption):\n",
    "    return nltk.translate.bleu_score.sentence_bleu([ref_caption], gen_caption,  weights=(1, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_captions(image_num):\n",
    "    image = img_name_val[image_num]\n",
    "    real_caption = [tokenizer.index_word[i]\n",
    "                        for i in cap_val[image_num] if i not in [0]]\n",
    "    result, attention_plot = evaluate(image)\n",
    "    return real_caption, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bleu_1 = 0.0\n",
    "total_bleu_2 = 0.0\n",
    "total_bleu_3 = 0.0\n",
    "total_bleu_4 = 0.0\n",
    "for i in range(len(img_name_val)):\n",
    "    real_caption, gen_caption = get_captions(i)\n",
    "    total_bleu_1 += BLEU_1(real_caption, gen_caption)\n",
    "    total_bleu_2 += BLEU_2(real_caption, gen_caption)\n",
    "    total_bleu_3 += BLEU_3(real_caption, gen_caption)\n",
    "    total_bleu_4 += BLEU_4(real_caption, gen_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = len(img_name_val)\n",
    "print(\"bleu 1 \", total_bleu_1/num_val)\n",
    "print(\"bleu 2 \", total_bleu_2/num_val)\n",
    "print(\"bleu 3 \", total_bleu_3/num_val)\n",
    "print(\"bleu 4 \", total_bleu_4/num_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_latest_p37)",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
